{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sobol_seq in /opt/anaconda3/lib/python3.7/site-packages (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Authors: \n",
    "# E. Pan - Imperial College\n",
    "\n",
    "import pylab\n",
    "import numpy as np\n",
    "import scipy.integrate as scp\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import numpy.random as rnd\n",
    "from scipy.spatial.distance import cdist\n",
    "!pip install sobol_seq\n",
    "import sobol_seq\n",
    "from scipy.optimize import minimize\n",
    "eps  = np.finfo(float).eps\n",
    "import random\n",
    "import time\n",
    "matplotlib.rcParams['font.sans-serif'] = \"helvetica\"\n",
    "matplotlib.rcParams['font.family'] = \"helvetica\"\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from IPython.display import Audio, display # Import sound alert dependencies\n",
    "def Done():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "# Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyanmical System\n",
    "\n",
    "Here we have a dynamic system, where $x$ is biomas, $N$ is nitrogen source, and $F_{N_{in}}$ is an inflow rate.\n",
    "\n",
    "$$\\frac{\\text{d}x}{\\text{d}t}=\\mu~x~\\frac{N}{N+K_N}-\\mu_d~x$$\n",
    "\n",
    "$$\\frac{\\text{d}N}{\\text{d}t}=-Y_{Nx}x~\\mu~\\frac{N}{N+K_N}+F_{N_{in}}$$\n",
    "\n",
    "We wish to optimize at the final time $t_f$ the cost given by the biomass we can harves, minus a penalization for the nitrogen that was not consumed, the objective function is:\n",
    "\n",
    "$$f_{obj_1}=100x_{t_f}-N_{t_f}$$\n",
    "\n",
    "where $x_{t_f}$ and $N_{t_f}$ refer to the final biomass and nitrogen quantity. We can control $F_{N_{in}}\\in[0,7]$ with a precision of $0.5$ changing it $10$ times (equidistantly) from time $0$ to time $t_f$\n",
    "\n",
    "We can aditionally have an objective function that penalizes nitrogen source input:\n",
    "\n",
    "$$f_{obj_2}=100x_{t_f}-N_{t_f}-\\sum_{i=0}^{T}F_{N_{in}}^i$$\n",
    "\n",
    "where $T$ is the total number of time steps (inputs) and $F_{N_{in}}^i$ corresponds to the nitrate input at time-step $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming solutions\n",
    "\n",
    "To solve this problem, initially we should think of discretizing states (time is already discretized). This can be done for Biomass in $0.5$ intervals, and for Nitrate in $25$ intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Defining Environment ##############\n",
    "\n",
    "class Model_env: \n",
    "    \n",
    "    # --- initializing model --- #\n",
    "    def __init__(self, parameters, tf, modulus):\n",
    "        \n",
    "        # Object variable definitions\n",
    "        self.parameters       = parameters\n",
    "        self.tf, self.modulus = tf, modulus  # two column array [biomass nitrate]\n",
    "        \n",
    "    # --- dynamic model definition --- #    \n",
    "    # model takes state and action of previous time step and integrates -- definition of ODE system at time, t\n",
    "    def model(self, t, state):\n",
    "        # internal definitions\n",
    "        params = self.parameters\n",
    "        FCn   = self.u0\n",
    "                \n",
    "        # state vector\n",
    "        Cx  = state[0]\n",
    "        Cn  = state[1]\n",
    "        \n",
    "        # parameters\n",
    "        u_m  = params['u_m']; K_N  = params['K_N'];\n",
    "        u_d  = params['u_d']; Y_nx = params['Y_nx'];\n",
    "        \n",
    "        # algebraic equations\n",
    "        \n",
    "        # variable rate equations\n",
    "        dev_Cx  = u_m * Cx * Cn/(Cn+K_N) - u_d*Cx**2\n",
    "        dev_Cn  = - Y_nx * u_m * Cx * Cn/(Cn+K_N) + FCn\n",
    "        \n",
    "        return np.array([dev_Cx, dev_Cn],dtype='float64')\n",
    "    \n",
    "    def discrete_env(self, state):\n",
    "        # discretisation of the system, with introduction of stochasticity in terms of modulus\n",
    "        modulus = self.modulus\n",
    "        \n",
    "        # passing to arrays\n",
    "        modulus = np.array(modulus)    # eg. modulus = np.array([0.2, 20.]) basically what modulus does is it indicates the rounding of the conc of x and conc of nitrate\n",
    "        state   = np.array(state)  # eg. state = np.array([0.1,150.0]) first number is the conc of x and 2nd number is the conc of nitrate\n",
    "\n",
    "        resid = state % modulus \n",
    "        resid = resid/modulus # remember resid is now an array. resid is normalized with respect to the size of modulus \n",
    "        LB = resid.copy()\n",
    "        UB = 1 - resid # 1 minus resid because we are talking about normalized inverse length\n",
    "        draw = [0,0]\n",
    "        for i in range(state.shape[0]):\n",
    "            if LB[i] < UB[i]: #if lower bound distance is smaller (more likely to round down)\n",
    "                LB[i] = LB[i]**3 #make distance even closer\n",
    "                draw[i] = np.random.uniform(0,LB[i]+UB[i],1)\n",
    "            elif LB[i] > UB[i]: # else upper bound distance is smaller (more liekly to round up)\n",
    "                UB[i] = UB[i]**3\n",
    "                draw[i] = np.random.uniform(0,LB[i]+UB[i],1)\n",
    "            else:\n",
    "                draw[i] = np.random.uniform(0,1,1)        \n",
    "        for i in range(state.shape[0]):\n",
    "            if draw[i] < UB[i]: #introduce stochasticity w.r.t. rounding up or down\n",
    "                state[i] = state[i] - resid[i] * modulus[i] #rmb now resid is now a NORMALIZED array of 2 numbers so it has to be multiplied back to un-normalize residual #rounds down by substracting away the residual\n",
    "            else:\n",
    "                state[i] = state[i] - resid[i] * modulus[i] + modulus[i] #rounds up\n",
    "        \n",
    "        # fixes for representation \n",
    "        # Nitrate fix\n",
    "        if state[1] < 0:\n",
    "            state[1] = 0\n",
    "        elif state[0] < 0:\n",
    "            state[0] = 0\n",
    "        \n",
    "        # Biomass fix\n",
    "        f = str(self.modulus[0])\n",
    "        decimal = f[::-1].find('.')  \n",
    "        state[0] = np.round(state[0], 2) # Changed the rounding to allow smaller x grid size\n",
    "        # used to be: state[0] = np.round(state[0], decimal)\n",
    "        f1 = str(self.modulus[1])\n",
    "        decimal1 = f1[::-1].find('.')  \n",
    "        state[0] = np.round(state[0], 2) # Changed the rounding to allow smaller x grid size\n",
    "        # used to be: state[0] = np.round(state[0], decimal1)\n",
    "\n",
    "        if state[0] == eps:\n",
    "            state[0] = 0\n",
    "        if state[1] == eps:\n",
    "            state[1] = 0\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def simulation(self, x0, controls):\n",
    "        # internal definitions\n",
    "        model, tf     = self.model, self.tf\n",
    "        self.controls = controls\n",
    "        \n",
    "        # initialize simulation\n",
    "        current_state = x0\n",
    "        \n",
    "        # simulation #ONLY ONE STEP unlike the previous code shown above\n",
    "        self.u0   = controls[:]                       # control for this step\n",
    "        ode       = scp.ode(model)                      # define ode\n",
    "        ode.set_integrator('lsoda', nsteps=3000)        # define integrator\n",
    "        ode.set_initial_value(current_state, tf)         # set initial value\n",
    "        current_state = list(ode.integrate(ode.t + tf)) # integrate system\n",
    "        xt            = current_state                   # add current state Note: here we can add randomnes as: + RandomNormal noise\n",
    "        \n",
    "        return xt\n",
    "\n",
    "    def MDP_simulation(self, x0, controls): #simulate ONLY ONE STEP\n",
    "        xt          = self.simulation(x0, controls) #simulate\n",
    "        xt_discrete = self.discrete_env(xt) # make output state discrete\n",
    "        return xt_discrete\n",
    "\n",
    "    def reward(self, state):\n",
    "        reward = 100*state[-1][0] - state[-1][1]              # objective function 1\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1.05)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modulus = [0.05  ,5.]\n",
    "f = str(modulus[0])\n",
    "decimal = f[::-1].find('.')  \n",
    "# state[0] = np.round(state[0], decimal) \n",
    "# f1 = str(self.modulus[1])\n",
    "# decimal1 = f1[::-1].find('.')  \n",
    "# state[0] = np.round(state[0], decimal1)\n",
    "decimal, np.round(1.05, decimal) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating one Step of the dynamic system as a MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "# p        = {'u_m' : 0.0923*0.62, 'K_N' : 393.10, 'u_d' : 0.01, 'Y_nx' : 504.49} # predetermined parameters by design\n",
    "# tf       = 16.*24./10. # assuming 10 steps, we divide the whole horizon over 10 for one step\n",
    "# x0       = np.array([0.2,700.0]) # initial state\n",
    "# modulus  = [0.1, 60.] # basically what modulus does is it indicates the rounding of the conc of x and conc of nitrate\n",
    "# u0       = np.array([7.]) #this is your CONTROL (rmb here it's only ONE STEP)\n",
    "\n",
    "# MDP_BioEnv = Model_env(p, tf, modulus)\n",
    "# MDP_BioEnv.MDP_simulation(x0, u0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating multiple steps of the dynamic system as a MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4d1de4feb403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFCn_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mMDP_BioEnv\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mModel_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodulus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mu0\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFCn_0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx0\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mMDP_BioEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMDP_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#simulate one step, and update current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "steps_      = 10\n",
    "FCn_0       = [np.sin(i/6.5)*2+5. for i in range(steps_)]\n",
    "x0          = np.array([0.2,150]) # initial state\n",
    "tf          = 16.*24./10.\n",
    "x_list      = np.zeros((2,steps_+1)) #initialize state as 2D array where the 2 rows refer to conc of x and N.\n",
    "t_list      = np.zeros((1,steps_+1)) #initialize time as 1D array\n",
    "x_list[:,0] = x0\n",
    "t_list[0]   = tf\n",
    "\n",
    "for s in range(len(FCn_0)):\n",
    "    MDP_BioEnv    = Model_env(p, tf, modulus)\n",
    "    u0            = np.array([FCn_0[s]])\n",
    "    x0            = MDP_BioEnv.MDP_simulation(x0, u0) #simulate one step, and update current state \n",
    "    tf            = tf + 16.*24./10. #increment time\n",
    "    x_list[:,s+1] = x0 #update state memory\n",
    "    t_list[:,s+1] = tf #update time memory\n",
    "\n",
    "for i in range(x_list.shape[0]):\n",
    "    plt.plot(t_list[0,:],x_list[i,:])\n",
    "    plt.ylabel('state')\n",
    "    plt.xlabel('time')\n",
    "    plt.show()\n",
    "plt.step(t_list[0,:-1],FCn_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.2,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,\n",
       "          0. ,   0. ],\n",
       "       [150. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,\n",
       "          0. ,   0. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and describe system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discrete_states: 140901\n",
      "Shape of value table: 701 x 201\n",
      "Number of possible actions: 8\n",
      "Number of possible times: 11\n"
     ]
    }
   ],
   "source": [
    "p        = {'u_m' : 0.0923*0.62, 'K_N' : 393.10, 'u_d' : 0.01, 'Y_nx' : 504.49} # predetermined parameters by design\n",
    "tf       = 16.*24./10. # assuming 10 steps, we divide the whole horizon over 10 for one step\n",
    "\n",
    "modulus  = [0.01, 1.] # basically what modulus does is it indicates the rounding of the conc of x and conc of nitrate\n",
    "\n",
    "Number_of_possible_x      = int(2.0/modulus[0]+1) #assumed max x value is 1.6\n",
    "Number_of_possible_N      = int(700/modulus[1]+1) #assumed max N value is 400\n",
    "Number_of_discrete_states = int(Number_of_possible_x * Number_of_possible_N)\n",
    "max_increase_N = 7 # Maximum increase in N per time step\n",
    "increment_N    = 1 # Spacing between increase in N\n",
    "Number_of_possible_actions = int(max_increase_N/increment_N + 1)\n",
    "Number_of_possible_times   = 11\n",
    "\n",
    "MDP_BioEnv = Model_env(p, tf, modulus) # Initialize system\n",
    "\n",
    "print('Number of discrete_states:', Number_of_discrete_states)\n",
    "print('Shape of value table:', Number_of_possible_N , 'x', Number_of_possible_x)\n",
    "print('Number of possible actions:', Number_of_possible_actions)\n",
    "print('Number of possible times:', Number_of_possible_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to map action to actual increase in N\n",
    "action_ID = [x for x in range(Number_of_possible_actions)]\n",
    "increase_in_N = [x for x in np.arange(0,max_increase_N+increment_N,increment_N)]\n",
    "action_ID_to_increase_in_N = dict(zip(action_ID,increase_in_N))\n",
    "action_ID_to_increase_in_N #this dictionary maps action id to actual action itself (increase in N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action): # state can be described by tuple(x (algae conc), N (nitrate conc), time (in hours)), action can be described by FN âˆˆ[0,7]\n",
    "    '''arguments\n",
    "       state: (x, N, t) tuple \n",
    "       action: int (0-14)\n",
    "    \n",
    "       outputs \n",
    "       new state: (x, N, t) tuple\n",
    "       reward: int \n",
    "       '''\n",
    "    state = np.array(state) \n",
    "    action = [action_ID_to_increase_in_N[action]] #assign action according to dictionary defined above\n",
    "    \n",
    "    if (abs(state[2] - 16.*24.) < 0.1): #check if terminal state is reached\n",
    "        reward = (100 * state[0] - state[1])   #give reward when we LEAVE terminal state\n",
    "        state[2]      += 16.*24./10. #increment time\n",
    "        state_evolved = MDP_BioEnv.MDP_simulation(state[0:2], action) #evolve the system using ODE\n",
    "        state_evolved = [round(state_evolved[0],2),round(state_evolved[1],1)]\n",
    "        new_state = np.append(state_evolved,round(state[2],2)) # append time\n",
    "        \n",
    "    elif (state[2] > 16.*24.): #check if time is exceeded\n",
    "        new_state = state #loop back to itself\n",
    "        reward = 0 #and given zero reward    \n",
    "\n",
    "    else: #else evolve the system and assign zero reward\n",
    "        state[2]      += 16.*24./10. #increment time\n",
    "        state_evolved = MDP_BioEnv.MDP_simulation(state[0:2], action) #evolve the system using ODE\n",
    "        state_evolved = [round(state_evolved[0],2),round(state_evolved[1],1)]\n",
    "        new_state = np.append(state_evolved,round(state[2],2)) # append time\n",
    "        reward    = 0 # if terminal step not reached, reward = 0\n",
    "\n",
    "    return tuple(new_state), reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.3, 87.0, 38.4), 0)\n",
      "((0.32, 116.0, 38.4), 0)\n",
      "((0.34, 146.0, 38.4), 0)\n",
      "((0.35, 176.0, 38.4), 0)\n",
      "((0.36, 206.0, 38.4), 0)\n",
      "((0.38, 237.0, 38.4), 0)\n",
      "((0.39, 268.0, 38.4), 0)\n",
      "((0.41, 299.0, 38.4), 0)\n"
     ]
    }
   ],
   "source": [
    "# Test transition function\n",
    "for i in range(Number_of_possible_actions):\n",
    "    print(transition((0.2,150,0),i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate 1 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current policy: [5, 7, 4, 1, 1, 2, 1, 2, 1, 3, 2]\n",
      "Concentration of x: [0.2, 0.38, 0.74, 1.03, 0.9, 0.73, 0.66, 0.61, 0.59, 0.56, 0.59, 0.63]\n",
      "Concentration of N: [150, 237.0, 262.0, 109.0, 23.0, 19.0, 40.0, 26.0, 45.0, 30.0, 69.0, 55.0]\n",
      "\n",
      "Episode: [[(0.2, 150, 0), 5, 0], [(0.38, 237.0, 38.4), 7, 0], [(0.74, 262.0, 76.8), 4, 0], [(1.03, 109.0, 115.2), 1, 0], [(0.9, 23.0, 153.6), 1, 0], [(0.73, 19.0, 192.0), 2, 0], [(0.66, 40.0, 230.4), 1, 0], [(0.61, 26.0, 268.8), 2, 0], [(0.59, 45.0, 307.2), 1, 0], [(0.56, 30.0, 345.6), 3, 0], [(0.59, 69.0, 384.0), 2, -10.0]]\n",
      "Score: -10.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3yV9d3/8dcng4SREAiBEDLYW4EQVsGBA2exOINab6tivW2tP7vuWu9W27u2dtqpraPiYmiVWsWttQ4EEoisgIhAJiOMBAgJWd/fHzlggADZ1xnv5+ORR0+uXOecT74Nb6/z/V6f6zLnHCIiEvjCvC5ARETahgJdRCRIKNBFRIKEAl1EJEgo0EVEgkSEl2/eq1cv179/fy9LEBEJOCtWrNjlnEs4drungd6/f3+ys7O9LEFEJOCYWV5j2zXlIiISJBToIiJBQoEuIhIkPJ1DFxHxQnV1NYWFhVRWVnpdyklFR0eTnJxMZGRkk/ZXoItIyCksLCQmJob+/ftjZl6X0yjnHLt376awsJABAwY06TmachGRkFNZWUl8fLzfhjmAmREfH9+sTxEKdBEJSf4c5oc1t0YFup8pLq1gUU4huqyxiDSXAt2PvLFuOxf94QPuWriK1YVlXpcjIu3kvffeIyIigtzc3CPb7rvvPv7617+26nUV6H6gsrqW+/61jq8/vYLE2GgAVubv9bgqEWlP3bp147vf/W6bvqYC3WOflxzg8oeWMHfJVm6eNoCX75hGYmw0K/NLvS5NRNrRBRdcwK5du3jrrbfa7DV12qKHXlhRyI9eWktURBh/vzGDc4b3ASA9LY4cHaGLdIifvLyO3OJ9bfqaI5NiuffLo066j5nx4IMPcvvtt5OTk9Mm76tA90D5oRp+9M+1vJhTxKQBPflD5jgSu0cf+Xl6ag9eXbOdnfsr6R0TfZJXEpFANnXqVIYPH84TTzzRJq+nQO9ga4vKuGN+Dnm7y7nrvKF885zBhIcdfWrSuNQeAKzMK+XC0YlelCkSMk51JN3efvWrXzF9+nSuuOKKVr+W5tA7iHOOuR9t4fKHllBRVcv8OZO587whx4U5wOh+sXQKD9O0i0gISEtLIzMzk8cee6zVr3XKQDezMDN7wszyzWyVmU01s2vNbLOZbfB9TfPt+yPffmvM7PRWVxck9pZXMeepFdz3ci5nDu3Fa3eewaSB8SfcPyoinFH9YnWmi0iIuPvuu4mObv30alOmXGYBPYE0YASwAHgBuMU59+7hncxsDHAJMBAYD/wROLvVFQa45Vv2cOeCHHYdOMSPLx3J16Y27doR6ak9eGZpHlU1dXSK0AcpkWBy9tlnc/bZZx/5PiYmhm3btrX6dZuSFInAk65eLpAApAIFx+x3MfCsc67GObcMSDGzLq2uMEDV1jn++M5nZD7yMVERYbz431O5adqAJrfypqf24FBNHeu3te3qu4gEr1MeoTvn/nL4sZnNAT6j/mj9ITNLBt4Gvk19yL/Z4Kk7qA//o26VZGa3ArcCpKamtrJ8/7RjXyX/b8EnfLx5N18Zm8TPZp1Gt6jmrT+np8UB9Q1GY1Li2qNMEQkyTfosb2ZdzexR4E7gRuA93+PTgDjgG4ADaho8zQG1x76Wc+4R51yGcy4jIeG4e5wGvH9/upOL/vABnxSU8purxvDgNWObHeYAfbt3pm93NRiJtJdAuF5Sc2s8ZdL4pk0+AP4DZACHgN875w74fj4fuBzYAiQ1eGpPYGezqglgVTV1/PqNDTz6wRaGJ8bw52vTGdy7W6teMz21ByvztDAq0taio6PZvXu3X19C9/D10JuzWNqUQ8fbgA+dc3cBmFkEsNHMMpxzxcBMYBmQBfzOdyR/BvCZc66qub9EIMrbXc4d83NYXVjGDVPS+OHFI4iODG/1645LjWPxmm3s3FdJ71g1GIm0leTkZAoLCykpKfG6lJM6fMeipmpKoI8HzjSzGQ223Qm8Z2Y11B+9z3XOVZvZB8AmoBS4uullB66XVxVz94trCDP46/Xj27QR6EiDUf5eLhzdt81eVyTURUZGNvkuQIGkKYui153gR883su+9wL2tLSoQVFTV8pOX17Egq4DxaT34Q+ZYknu07Uk9XzQYlSrQReSU1PrfAhu27+OOeTlsKjnAN6YP4q7zhhIR3vbniqvBSESaQ4HeDM455i3P56cv5xLbOZKnb5rEtCG92vU91WAkIk2lhGiisopqvjkvh3sWrWXSwHhe/dYZ7R7moAYjEWk6HaE3QU7+Xu6Yn8P2skruvmg4c84YSFgjF9VqD2owEpGm0hH6Kew6cIjZjy4F4LnbpvD1swZ1WJiDGoxEpOl0hH4KL64spLK6jidunMCQPjGe1KAGIxFpCh2hn4RzjgVZBWSk9fAszKG+waiotIKd+yo9q0FE/J8C/SSy8/ayuaScayakeFpHetoXDUYiIieiQD+JBcsL6BYVwSWne9vUMyqpvsFI8+gicjIK9BPYV1nN4jXFzBybRJdO3i41HGkw0jy6iJyEAv0E/vVJMZXVdWR6PN1yWHpqD9YUlVFVU+d1KSLipxToJ7Awq4CRfWM5rV93r0sB1GAkIqemQG/E2qIy1hSVkTkxxW+uldywwUhEpDEK9EYszCogKiKMy8b087qUI9RgJCKnokA/RkVVLf/8pIiLT+tL9y6RXpdzFDUYicjJKNCP8drabeyvrPH83PPGqMFIRE5GgX6MBVkF9I/vwqQBPb0u5ThqMBKRk1GgN7C55ADLt+zhmgmpfrMY2pAajETkZBToDSzMLiA8zLhivP8shjYUFRHOaDUYicgJKNB9qmvreGFFIecO703vmGivyzmh9NQerFaDkYg0QoHu8876new6UEXmRP9bDG0oPa0HVTV15KrBSESOoUD3WZiVT2JsNGcOSfC6lJMal1rfYJSjhVEROYYCHSgureA/G0u4KiOZiHD/HhI1GInIifh3enWQ57MLccDVGf493XKYGoxEpDEhH+i1dY7nsguYNrgXKT27eF1Ok6jBSEQaE/KB/tGmXRSVVvhlZ+iJqMFIRBoT8oG+MKuAHl0iOX9kH69LaTI1GIlIY0I60HcfOMSbudu5PD2ZqIhwr8tpMjUYiUhjQjrQF+UUUV3rAmq65TA1GInIsUI20J1zLMgqID01jqF9Yrwup9nUYCQixwrZQF+Zv5dNOw+QOSHV61JaJD3VtzCqaRcR8QnZQF+wvICuncK55PS+XpfSIondo0nqHq0zXUTkiFMGupmFmdkTZpZvZqvMbKqZTTCzXDPbambf8e1nZvawmeWZ2VIzS27/8ltmf2U1r6zexsyxSXSNivC6nBYbl9aDHJ3pIiI+TTlCnwX0BNKA2cDDvq/ZwBDgejMbCMwEEoD+wIPA/e1Qb5v416piKqpruSZAp1sOG5dS32C0Qw1GIkLTAj0ReNLVywVOA8w5t8o5Vw28BJwPXHx4P2ARcHY71dxqC7MKGJ4Yw5jk7l6X0iqHG4x0oS4RgSYEunPuL865FwHMbA5QCRQ22KWY+tBPPbzdOVcFhJvZca9vZreaWbaZZZeUlLTBr9A864rLWF1YRuaEFL+8K1FzqMFIRBpq0qKomXU1s0eBO4GzgJoGP3ZAre9/G26vcc4dd5K0c+4R51yGcy4jIaHjL1X7XFYBnSLC+Mo4/7wrUXOowUhEGmrKomgX4APgAJABFAFJDXZJAvIbbjezSOqP5P1KZXUti3KKuGh0InFdOnldTptQg5GIHNaUI/TbgA+dc3c55yqdc0VAhJkNNbOu1C+GvgUsBq71Peda3za/8vra7eyrrAnIztATUYORiBzWlHP2xgNnmtmMBttuBF4EugI/d85tM7OXgEvNbDNQAFzR1sW21oKsfNLiuzB5QLzXpbSZhg1GY1PiPK5GRLx0ykB3zl13gh+NPmY/B9zSFkW1hy27ylm6eQ/fu2AYYWGBvRjaUMMGo5sY4HU5IuKhkOkUfS67gPAw48rxftvv1GJqMBIRCJFAr66t4x8rCpk+rDd9YqO9LqfNpaf2UIORiIRGoL+7YScl+w+RGUSLoQ2lp9bPnev0RZHQFhKBvjCrgN4xUZw9rOPPe+8Io5K60ykijJwCTbuIhLKgD/RtZRW89+lOrspIJiI8OH/dThFhjE5Sg5FIqAvOhGvgH9mF1Dm4JiOwL8R1KmowEpGgDvS6OsfC7AKmDo4nNb6L1+W0KzUYiUhQB/qSz3dTuLci4C+T2xS6g5GIBHWgL8jKJ65LJDNG9vG6lHanOxiJSNAG+p7yKt5ct4NZ4/oRHRnudTkdQg1GIqEtaAN9UU4RVbV1QXUhrlNRg5FIaAvKQHfOsTArn7EpcQxPjPW6nA6jBiOR0BaUgb4yv5SNOw4EbWfoiRxuMNI8ukhoCspAX5iVT5dO4Vw6JunUOweRThFhnNavu25JJxKigi7Q91dW8/KqbXz59CS6RTXlcu/BJT01jjVqMBIJSUEX6K+s3kZFdS3XTAyt6ZbD0lPVYCQSqoIu0BdkFTC0TzfGhejde8apwUgkZAVVoK/fto9VBaVkTkjFLHjuStQcajASCV1BFegLswroFB7GrHH9vC7FU2owEglNQRPoldW1LMop4oLRifTo2snrcjylBiOR0BQ0gf7Guu2UVVSH3LnnjVGDkUhoCppAX5hVQErPzkwZGO91KZ5Tg5FIaAqKQM/bXc6Sz3dzTUYKYWGhuRjakBqMREJTUAT6wqwCwgyuHK/plsPUYCQSegI+0Gtq63h+RSHTh/UmsXu01+X4jcMNRuuKy7wuRUQ6SMAH+r8/LaFk/6GQukxuU6Sn+RqMNO0iEjICPtAXZuWTEBPF9OG9vS7Fr/SJjaZfXGdytDAqEjICOtC3l1Xy7oadXDk+mcjwgP5V2sW41Dg1GImEkIBOwRdWFlLn4JoMTbc0ZpwajERCSsAGel2dY2FWAVMGxtO/V1evy/FLajASCS0BG+hLN+8mf89BMkP0MrlNoQYjkdDS5EA3s8vM7AHf42vNbLOZbfB9TfNt/5GZ5ZvZGjM7vb2KhvrL5HbvHMkFoxLb820CmhqMRELLKQPd6j0IPNJg8xDgFufccN/Xh2Y2BrgEGAjcAvyxXSoG9pZX8fra7cwa14/oyPD2epugoAYjkdDR1CP0d4B5Db5PBQqO2edi4FnnXI1zbhmQYmZd2qDG4yzKKaKqtk7nnjeBGoxEQscpA93VewVY1WBzGvCQma03sz+ZWST1IV/YYJ8dQMKxr2dmt5pZtplll5SUtKjoN9ZtZ0xyd0b0jW3R80OJGoxEQkdLF0XfA+4ETgPigG8ADqhpsI8Dao99onPuEedchnMuIyHhuLxvkqdunsifZqe36Lmh5nCDkRZGRYJfswPd6u/t9nvnXK5zrgaYD4wGioCkBrv2BHa2SZXHiIoIJzW+XWZzgtK41DhydOqiSNBryRF6OLDRzA6H90xgGbAYuMbMwszsLOAz51xVG9UprZCe2oPiskq2l6nBSCSYNTvQfUfldwLvmVkuYMBc59wnwAfAJuBB4NttWai03OF5dF3XRSS4RTR1R+fc3AaPnweeb2Sfe4F726QyaTMj+8YeaTC66LS+XpcjIu0kYDtFpenUYCQSGhToIUINRiLBT4EeItRgJBL8FOghQg1GIsFPgR4i1GAkEvwU6CFEDUYiwU2BHkLUYCQS3BToIeSLeXQdpYsEIwV6CBnZN5aoiDDdkk4kSCnQQ8jhBqOcAp3pIhKMFOghJj2thxqMRIKUAj3EjEuJU4ORSJBSoIcYNRiJBC8FeohRg5FI8FKghyA1GIkEJwV6CFKDkUhwUqCHIDUYiQQnBXoIUoORSHBSoIegL+5gpEAXCSYK9BCVntaDtUX7OFRT63UpItJGFOghKj01jqraOnKL93ldioi0EQV6iEpPVYORSLBRoIeo3mowEgk6CvQQpgYjkeCiQA9hajASCS4K9BCmBiOR4KJAD2FqMBIJLgr0ENYpIozTk7uzbMser0sRkTagQA9xF4xKZE1RGZ9u3+91KSLSSgr0EHd5ejKdwsOYvzzf61JEpJUU6CGuZ9dOXDA6kUU5RVRW6zIAIoFMgS7MnpBCWUU1r6/d7nUpItIKTQ50M7vMzB7wPZ5gZrlmttXMvuPbZmb2sJnlmdlSM0tur6KlbU0eGE//+C7M07SLSEA7ZaD7gvpB4JEGmx8GZgNDgOvNbCAwE0gA+gMPAve3ebXSLsLCjGsmpLJ8yx4+LzngdTki0kJNPUJ/B5gHYGZ9AXPOrXLOVQMvAecDFwNPOuccsAg4u+3LlfZy5fhkIsKMhVkFXpciIi10ykB39V4BVvk2pQKFDXYpBhIbbnfOVQHhZnbc65vZrWaWbWbZJSUlra1f2khCTBTnjejDP1YU6hrpIgGqJYuiDqg55vvaRrbXOOfqjnuyc4845zKccxkJCQkteHtpL7MnpbKnvIq3cnd4XYqItEBLAr0ISGrwfRKQ33C7mUUCuuJTgDljcC/6xXVmwXJNu4gEomYHunOuCIgws6Fm1pX6xdC3gMXAtb7drvVtkwBSvziawoebdpG/+6DX5YhIM7X0PPQ7gBeBtcBfnXPbqF8crTazzcBNwE/apkTpSFdlJBNmsCBLpzCKBJqIpu7onJvb4PFSYPQxP3fALW1WmXiib/fOnDO8N8+vKOSu84cSGa7eM5FAoX+tcpzMCamU7D/Euxt2el2KiDSDAl2Oc/awBPrERrFAnaMiAUWBLseJCA/j6owU3ttYQlFphdfliEgTKdClUVdnpADwnDpHRQKGAl0aldKzC2cMSeD57AJq65zX5YhIEyjQ5YRmT0ihuKyS9zfqEg0igUCBLid07og+9OrWSZfVFQkQCnQ5oU4RYVw5PoV3N+xk5z5dyUHE3ynQ5aQyJ6RQW+d4fkXhqXcWEU8p0OWk+vfqypSB8SzIyqdOi6Mifk2BLqeUOTGFgj0VfPT5Lq9LEZGTUKDLKV0wKpEeXSJ1WV0RP6dAl1OKjgzn8vRk3szdzu4Dh7wuR0ROQIEuTTJ7YgrVtY4XVmpxVMRfKdClSQb3jiEjrQcLlhdQf6VkEfE3CnRpstkTU9m8q5xlW/Z4XYqINEKBLk128Wl9iYmO0GV1RfyUAl2arHOncGaN68era7dTerDK63JE5BgKdGmWzAmpVNXU8eLKIq9LEWkTzjmeyy5gyee7An59qMn3FBUBGJkUy5iUOBZk5fO1qf0xM69LEmmVP7+7id++tRGAQQlduW5SGleMT6Z750iPK2s+HaFLs82ekMLGHQdYmV/qdSkirfLiykJ++9ZGZo3rx2+vGkNs50h++kouk37+Nv/zj9WsKSzzusRm0RG6NNuXxyTxf6/kMn95PuPTenhdjkiLLNm0i/95YTVTBsbzyytOp1NEGFeMT2ZtURnPLsvjnznFLMwuYExKHNdPSuXLY5KIjgz3uuyTMi/njDIyMlx2drZn7y8td/eLq1mUU8Tye84jNjrwPppKaNu4Yz9XPLyEvt2jef62LzU6vbKvspoXVxTyzLJ8Nu08QPfOkVw1PpnrJqcxoFdXD6r+gpmtcM5lHLtdUy7SIrMnplJZXcdLnxR7XYpIs+zcV8nXnsgiOjKcv9844YRz5bHRkdw4dQBv3XUm8+dMZtqQXsxdspXpv3mPrz6+jNfXbqemtq6Dqz85TblIi5zWrzsj+8Yyf1k+109K1eKoBITyQzXc9GQWew9W8dzXp5Dco8spn2NmTBkUz5RB8ezcX8nC5QXMX57Pbc+sIDE2msyJKcyemEqf2OgO+A1OTkfo0iJmxuyJKeRu28eaosBaOJLQVFNbxzfnrSS3eB9/uTad0f26N/s1esdEc8e5Q3j/+9N59IYMhibG8Pu3P+NLD7zLfz+zgo82eXvqo47QpcUuG9eP+19dz/zlBZyeHOd1OSIn5Jzj3n+t49+flnD/rNFMH967Va8XER7G+SP7cP7IPmzdVc685fk8l13Aa2u3M9B36uOV6cl079Kx60s6QpcWi42O5NLTk/jXJ0WUH6rxuhyRE/rb+5t5dlk+t501iOsmpbXpa/fv1ZUfXjyCpXefy++uHkP3zpH83yu5TPrF23z/H6tYXdhxp/cq0KVVZk9MobyqlldWa3FU/NO/VhXzwGsb+PKYJL5/wbB2e5/D9w1YdPtUFn9rGrPGJfPyqm3M/PNHXPbnD3kuu4CKqtp2e3/QaYvSSs45Zjz4Pl2iInjpG1O9LkfkKMu37OH6x5YxNiWOp2+ZSFREx55Hvq+ymkUri3hmaR6f7TxAbHQEV2WkcN2kVAYmdGvx6+q0RWkXZkbmxFRWFZSyfts+r8sROeLzkgPMeSqb5J6deeSG8R0e5lA/LflfX+rPm3edyYJbJ3Pm0ASeXLKVc377Hz4paPupGAW6tNrl4/rRKSJMl9UVv1Gy/xA3PrGciDBj7o0TievSydN6zIzJA+P587XpLLn7HH586UhOb8FZNqfS4kA3s2wz2+D7WmBmA81spZnlm9nv2rJI8W89unbiotGJLMopavc5QpFTqaiq5ZansinZf4jHb5xAavypzzXvSL1jorlp2gDCwtq+d6NFgW5m4cA+59xw31cm8FvgXiANGGZm09uwTvFzmRNS2VdZw6trtnldioSw2jrHtxbksLqwlD9mjmNsSmidTtvSI/Qk4Mi/XF/AjwdecfWrrM8DM1pfngSKyQN7MqBXVxZkadpFvPN/r+TyVu4O7r10JDNGJXpdTodraaCnAWPMbJWZLQOmAXvdF6fMFAONjqaZ3eqbrskuKSlp4duLvzEzMiekkLV1L5t27ve6HAlBj3+4hblLtnLztAHcOHWA1+V4oqWBXg48BmQAtwGvAw07SxzQ6GSqc+4R51yGcy4jISGhhW8v/uiK8clEhhsLlhd4XYqEmNfWbONni3O5aHQi91w8wutyPNPSQN8A/Nk5V+2cywHWAmMa/DwJ0GfvENOrWxTnj+zDCysLOVSjxVHpGCvy9vL/Fn7CuJQ4HrxmbLssNgaKlgb6ncAvAcxsEBALvGpm033z6dcDi9umRAkkmRNS2XuwmjfW7fC6FGkjJfsP8cNFa5j883e496W1bN1V7nVJR2zdVc6cp7JJ7B7Nozdk+P0NKNpbSy/O9WdgnpltAsqAm4EdwAIgHpjrnFvRNiVKIJk2uBfJPTqzYHk+M8ckeV2OtEJldS2Pf7iFh/69iUM1dUwZFM+85fk8tTSPc4f34eZpA5g8sKdnl07eU17FjU8sxznH3K9NJL5blCd1+JMWBbpz7gAws5EfjW9dORLowsLqF0d/8+ZGtu4qp7/Hd3aR5qurc7y0qohfv/4pxWWVzBjZhx9cNJyBCd3Yua+Sp5fm8czSPN5ev4ORfWO5edoAvjwmiU4RHdenWFldy5ynsikuq2T+nEme30HIX+haLtLmduyr5EsPvMucMwbyg4uGe12ONMPSzbu5f/F61hSVcVq/7txzyQgmD4w/br/K6loW5RTx9w+38NnOAyTERHHD5DSum5xGz67t25VZV+e4Y34Or67dxl+uTefi0/q26/v5oxNdy0WBLu3iliez+aSglI/vPofI8OC/wkTBnoPcv3g9JQcOcXVGMjPH9KNzp8CZz91ccoAHXtvAm7k7SOoezfcuHMZlY/qdcoHROcf7n+3i8Q+38P7GEqIiwpg1rh83TRvA0D4x7VLrz19dzyPvb+aei0cw58yB7fIe/k6BLh3q3Q07uGluNn+9Pp0LRwfvEVRVTR2PfrCZP737GeFmJMV1PnJVvSvGJ3PdpDQG9275VfXa297yKv7wzmc8szSPqIgwbp8+mJunDWjR4uJnO/bz94+28OLKIg7V1HHGkF7cPG0AZw1NaLN59qc+3sqPX1rHf01J476Zo0L21ocKdOlQtXWOab98lyF9Ynjqpolel9Mulm3ezT3/XMumnQe4cFQi984cSWJsNNl5e3n64zxeW7uN6lrHlwbFc/3kNM4f2cdvPq0cqqnlySVb+dO7myg/VEPmxFTuOm8oCTGtX1jcU17FvGV5PPVxHjv3H2Jw727cNHUAl6f3a9VZKG/n7uDWp7M5Z3gf/vbV8YSH8OmJCnTpcL97ayN/evcz3v/edFJ6+tcFklpjT3kVv3h1Pc+vKKRfXGd+etkozh3R57j9dh04xHPZBTy7NJ+i0gp6x0SROTGV2RNT6Nu9sweV10+RvLpmOw+8vp6CPRWcPSyBH148ol2mR6pq6nhldTGPf7iFdcX76NElkusmpXHDlDR6N/OGyqsLS7nmb0sZ0qcbC26dTJdOoX33TAW6dLjCvQc541f/5o7pg/n2jPa7U0xHqatzPL+igF+8toEDlTXMOXMg3zpnyCnnymvrHP/ZuJOnP87jvY0lhJlx7vDefHVKGlMH9eqwRpgVeXu5f3EuK/NLGZ4Ywz2XjOCMIe3fre2cY9mWPTz+4RbeXr+DiDDj0tOTuHnagCbdqLlgz0FmPfQR0ZHhLLp9apt8igh0CnTxxI1PLGfDtv18+D/TifCT6YaW+HT7fv73n2vI2rqXCf17cP+s01p0VFuw5yDzluezMKuAPeVV9I/vwvWT07hyfHK7XbO7YM9BHnh9A4tXbyMhJorvzhjKleNTPJmy2LqrnLlLtvJcdgEHq2qZOKAnN08bwHkj+jRaT9nBai5/+CN2Hajihf+ewuDe7bPQGmgU6OKJ19du57ZnVvDYDRmcN/L4aQl/d7Cqhj++s4nHPthMTHQEd188givTk1t9VH2oppbX127nmaV5ZG3dS1REGJeensRXp6QxJrl7myz2lVVU89C/N/HER1sJC4NbzxzE188cSNco76cryiqqWZiVz5NL8igqrSAtvgs3fqk/V2Wk0M1X36GaWm54fDk5+aU8ffNEJjVy+mSoUqCLJ6pr65jyi3cZm9Kdx/5rgtflNMs763fw45fWUVRawdUZyfzgohHtco71hu37eGZpHotWFlFeVcvofrFcPymNmWOTWjRXXF1bx7xl+fz+7Y2UVlRzRXoy350xjMTuzZu37gg1tXW8vm47j3+4hZz8UmKiI8ickMINU/rzmzc/5aVPivlD5lguG9vP61L9igJdPPPL1zfwt/98zpIfnOuXoXKs4qCipR4AAAb4SURBVNIKfvLyOt5Yt4Mhvbtx/6zTmDigZ7u/74FDNSzKKeLZpXls2L6fmOgIrkhP5vrJTTv10TnH2+t38ovX1rO5pJwvDYrnnktGMCqp7W911h5W5u/l7x9u4bW126mtq8+l7184jNvPHuxxZf5HgS6eydtdzlm/fo/vnD+UO84d4nU5J1RTW8fcJVv53VsbqXOOO88dys3TBnRoSzvUB/OKvL08vTSP19Zsp6q2jikD6099nDGq8VMf1xaV8bPFuSzdvIdBCV2555IRTB/WOyDP0y4qreDpj/Po2imcb54zOCB/h/amQBdPXffYUrbuOsgH35/ul5c3XZm/l3sWrWX9tn2cM7w3P5k5yi9OtTx86uO8ZfkU7q0gISaK2RNSyJyYSlJcZ7aVVfDrNz5lUU4RPbp04q7zhpA5MdVvzneX9qFAF0+9vKqYO+bn8ORNEzlrqP/c2KTsYDW/fGMD85fn0ycmmvtmjuSCUYl+d1RYW+d4f2MJTy/N49+f7sSAKYPiWZG3lzoHN00dwO3TBxEbHel1qdIBThTo3i93S0iYMaoPPbpEsmB5vl8EunOOf35SxP2L17OnvIqbpg7grvOHHjnDwt+EhxnTh/dm+vDeFOw5yPzl+by8upgZIxP53gXD/OLThHjPP/96JehERYRz5fhknvhoKzv3V9I7xrvF0c9LDvC/i9by8ebdjEmJY+7XJjapwcVfpPTswvcvHM73L9SVLOVoCnTpMJkTU3n0gy1M/vk79ImNJimus+8rmn5xnel35PvOxEZHtPm0R2V1LQ+99zl/fe9zoiLD+NlXRjN7YmpIXxNEgosCXTrMoIRuPHPzJJZv2U1RaSXFpRWsLizljbWVVNXWHbVvt6gIkuK+CP2jAz+aPrHRzVr4e39jCT96aS15uw/ylbFJ3HPJSLWQS9BRoEuHmjakF9OG9DpqW12dY1f5IYp9IV+0t4Ki0gqKSysoLqtgdWEZe8qrjnpOmNGko/yS/Yf46Su5vLJ6GwN6deXZWyYxdfDR7y8SLBTo4rmwMKN3TDS9Y6IZmxLX6D4VVbUUl/lC/kjon/oov7bOUescd503lK+fNTDkbyIswU2BLgGhc6dwBiV0Y1BC4x2Txx7lF5fWH+UfqqljzhkDdc9JCQkKdAkKTTnKFwl2aicTEQkSCnQRkSChQBcRCRIKdBGRIKFAFxEJEgp0EZEgoUAXEQkSCnQRkSDh6Q0uzKwEyGvh03sBu9qwnGCj8Tk5jc/JaXxOzuvxSXPOHXdjAU8DvTXMLLuxO3ZIPY3PyWl8Tk7jc3L+Oj6achERCRIKdBGRIBHIgf6I1wX4OY3PyWl8Tk7jc3J+OT4BO4cuIiJHC+QjdBERaUCBLiISJAIu0K3ew2aWZ2ZLzSzZ65q8ZmaXmdkDvscTzCzXzLaa2Xd820JuzMwszMyeMLN8M1tlZlM1Nl8wsxgze9nMPjWzlWY2XuNzPN/f0VIzu9DMBvrGKt/Mftdgnx/5tq0xs9O9rDfgAh2YCSQA/YEHgfs9rcZDvn9sD3L0As3DwGxgCHC9mQ0kNMdsFtATSKN+PB5GY9PQt4GlzrlhwP8CP0Hj05g7qR8PgN8C91L/NzXMzKab2RjgEmAgcAvwR0+q9AnEW9BdDDzpnHNmtgj4ldcFeeydww/MrC/1C92rfN+/BJwPpBN6Y5aI73cGcs3sNGClxuaIN4HNvsdxQGf0t3MUMxtA/Ri8AoQD44HLfWPxPDAD2Ac865yrAZaZWYqZdXHOHfSi5kA8Qk8FCgGcc1VAuJkF4u/Raq7eK8Aq36YjY+NTTH2whdyYOef+4px7EcDM5gCVaGyOcM597JzbYWY5wDPUB7zG52h/Au4CHPWt/nvdF6cFHjc+Pjuo/0TjiUD8P8YBNQ2+r3HO1XlVjJ85dmwcUNvI9pAYMzPramaPUv+x+Sw0Nsdxzo0DpgMPoPE5wsxuApY55z71bWrqv63D2z0RiIFeBCQBmFkk9UdeUu/I2PgkAfmE4JiZWRfgA+AAkIHG5ihm9uDhBU7n3H98mzU+Xzgb+KqZbaB+PeZ+oHeDnx83Pj49gZ0dVONxAjHQFwPX+h5fC7zlYS1+xTlXBESY2VAz60r9gtZbhOaY3QZ86Jy7yzlXqbE5TjhwNYCZTQGWoPE5wjl3g3NuqHNuOLAImAOs8C2EhgPXUz82i4FrfGfDnAV85pua8kQgLoq+BFxqZpuBAuAKj+vxN3cALwJdgZ8757b5FrhCbczGA2ea2YwG225EY3PY/cA8M7uN+svAzgFi0PiczPeABUA8MNc5twLAzD4ANgGl+P4j6RW1/ouIBIlAnHIREZFGKNBFRIKEAl1EJEgo0EVEgoQCXUQkSCjQRUSChAJdRCRI/H+PTF+g6i/CQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = (0.2,150,0) # Initial state\n",
    "x_data = [state[0]] # Store initial data \n",
    "N_data = [state[1]]\n",
    "t_data = [state[2]]\n",
    "my_policy = []\n",
    "episode = [] # Initialize episode for saving S, A, R values as list of lists\n",
    "\n",
    "\n",
    "for i in range(11): #take (10 + 1) steps\n",
    "    old_state = state # Old state for storing into episode\n",
    "    action = random.randint(0,Number_of_possible_actions - 1) # randint is end-exclusive\n",
    "    state, reward  = transition(state, action) # Evolve to get new state \n",
    "    episode += [[old_state, action, reward]] # Update step\n",
    "        \n",
    "    my_policy += [action]\n",
    "    x_data += [state[0]]\n",
    "    N_data += [state[1]]\n",
    "    t_data += [state[2]]\n",
    "print('Current policy:', my_policy)\n",
    "print('Concentration of x:', x_data)\n",
    "print('Concentration of N:', N_data)\n",
    "print('')\n",
    "print('Episode:', episode)\n",
    "score = transition((x_data[-2], N_data[-2],t_data[-2]),0)[1]\n",
    "print('Score:', score)\n",
    "def plot(t_data, x_data, N_data):\n",
    "    plt.figure()\n",
    "    plt.plot(t_data, x_data, label = 'x')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(t_data, N_data, label = 'N')\n",
    "    plt.legend()\n",
    "    \n",
    "plot(t_data, x_data, N_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_random_action(epsilon): # epsilon represents the probability of taking a random action\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q_table, epsilon): \n",
    "    '''Generates an episode with the chosen action of each step having:\n",
    "    Probability of epsilon       ---> random action\n",
    "    Probability of (1 - epsilon) ---> greedy action (according to Q table)\n",
    "    '''\n",
    "    episode = []\n",
    "    state = (0.2,150,0) # Initial state\n",
    "    for i in range(11): #take (10 + 1) steps\n",
    "        old_state = state # Old state for storing into episode\n",
    "        if old_state not in Q_table.keys(): # If state has NOT been visited before\n",
    "#             print('Unvisited')\n",
    "            action = random.randint(0,Number_of_possible_actions - 1) # Choose random action\n",
    "        else:                               # If state has been visited before\n",
    "#             print('Visited')\n",
    "            if take_random_action(epsilon): # Take random action\n",
    "                action = random.randint(0,Number_of_possible_actions - 1)\n",
    "            else:                           # Else take greedy action\n",
    "                action = np.argmax(Q_table[old_state])\n",
    "        \n",
    "        state, reward  = transition(state, action) # Evolve to get new state \n",
    "        episode += [[old_state, action, reward]] # Update step\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Q_table(episode, Q_table):\n",
    "    '''takes in an episode [[S0,A0,R1], [S1,A1,R2] .... ] --> a list of lists\n",
    "    and a Q table\n",
    "    and returns the updated Q table using incremental averaging\n",
    "    '''\n",
    "    Q_table['episode_count'] += 1 # Counter for num of episodes (for incremental average)\n",
    "    for step in reversed(range(11)): # Episode has 11 entries, and Q table is updated in reversed order\n",
    "        state, action, reward = episode[step]\n",
    "\n",
    "        if step == 10: # If terminal state i.e. t = 384\n",
    "            G = reward # Return = reward at terminal state\n",
    "        else:\n",
    "            G = reward + 0.9*G  # Return = reward + discounted return of the PREVIOUS state\n",
    "\n",
    "        if state not in Q_table.keys(): # If state has not been visited before\n",
    "#             print('Unvisited')\n",
    "            Q_table[state] = []         # Initialize state entry in Q table\n",
    "            for a in range(Number_of_possible_actions): # Iterate through all actions\n",
    "                if a == action: # If the action is indeed taken\n",
    "                    Q_table[state] += [G]\n",
    "                else:\n",
    "                    Q_table[state] += [-100000] # assign a default large negative value\n",
    "\n",
    "        else:                          # Else state has been visited before\n",
    "#             print('Visited')\n",
    "            for a in range(Number_of_possible_actions):\n",
    "                if Q_table[state][a] != -100000: # If this specific (s, a) has been updated before\n",
    "                    Q_table[state][a] = Q_table[state][a] + 1/Q_table['episode_count']*(G - Q_table[state][a]) #Incremental average update\n",
    "                elif Q_table[state][a] == -100000 and a == action: # If this state visited before but action not taken before\n",
    "                    Q_table[state][a] = G\n",
    "    return Q_table            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_q_table(Q_table, num_iterations):\n",
    "    total_score = 0\n",
    "    count = 0\n",
    "    for j in range(num_iterations):\n",
    "        state = (0.2,150,0) # Initial state\n",
    "        try:\n",
    "            for i in range(10): #take ten steps\n",
    "                    action = np.argmax(Q_table[state])\n",
    "                    state = transition(state, action)[0]\n",
    "            score = 100*state[0] - state[1]\n",
    "            count += 1\n",
    "            total_score += score\n",
    "        except:\n",
    "            pass\n",
    "    return total_score/count\n",
    "# score_q_table(Q_table1, num_iterations = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method in the cell below **works**\n",
    "Epsilon needs to be decayed gradually and exponentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table1 = {'episode_count': 0} # Initialize empty Q table as dictionary of list where INDEX corresponds to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.99     # Initialize epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes ran:  100\n",
      "Current epsilon:  0.98901\n",
      "Average error between current Q table and previous Q table:  0.0\n",
      "\n",
      "Episodes ran:  200\n",
      "Current epsilon:  0.98802099\n",
      "Average error between current Q table and previous Q table:  10477.987200650306\n",
      "\n",
      "Episodes ran:  300\n",
      "Current epsilon:  0.98703296901\n",
      "Average error between current Q table and previous Q table:  6277.506230911116\n",
      "\n",
      "Episodes ran:  400\n",
      "Current epsilon:  0.98604593604099\n",
      "Average error between current Q table and previous Q table:  4466.074550526848\n",
      "\n",
      "Episodes ran:  500\n",
      "Current epsilon:  0.985059890104949\n",
      "Average error between current Q table and previous Q table:  3926.2176879647823\n",
      "\n",
      "Episodes ran:  600\n",
      "Current epsilon:  0.984074830214844\n",
      "Average error between current Q table and previous Q table:  3928.4945774957205\n",
      "\n",
      "Episodes ran:  700\n",
      "Current epsilon:  0.9830907553846291\n",
      "Average error between current Q table and previous Q table:  3380.017433542591\n",
      "\n",
      "Episodes ran:  800\n",
      "Current epsilon:  0.9821076646292445\n",
      "Average error between current Q table and previous Q table:  3247.2904308435373\n",
      "\n",
      "Episodes ran:  900\n",
      "Current epsilon:  0.9811255569646152\n",
      "Average error between current Q table and previous Q table:  2961.775918373522\n",
      "\n",
      "Episodes ran:  1000\n",
      "Current epsilon:  0.9801444314076506\n",
      "Average error between current Q table and previous Q table:  2935.51108989182\n",
      "\n",
      "Episodes ran:  1100\n",
      "Current epsilon:  0.9791642869762429\n",
      "Average error between current Q table and previous Q table:  2713.243547842673\n",
      "\n",
      "Episodes ran:  1200\n",
      "Current epsilon:  0.9781851226892667\n",
      "Average error between current Q table and previous Q table:  2738.7869810782668\n",
      "\n",
      "Episodes ran:  1300\n",
      "Current epsilon:  0.9772069375665775\n",
      "Average error between current Q table and previous Q table:  2292.0003545278614\n",
      "\n",
      "Episodes ran:  1400\n",
      "Current epsilon:  0.976229730629011\n",
      "Average error between current Q table and previous Q table:  2405.082101862888\n",
      "\n",
      "Episodes ran:  1500\n",
      "Current epsilon:  0.9752535008983819\n",
      "Average score over 5k episodes:  64.3464925599757\n",
      "Average error between current Q table and previous Q table:  2529.7194391972057\n",
      "\n",
      "Episodes ran:  1600\n",
      "Current epsilon:  0.9742782473974836\n",
      "Average error between current Q table and previous Q table:  2419.8584047459562\n",
      "\n",
      "Episodes ran:  1700\n",
      "Current epsilon:  0.9733039691500861\n",
      "Average error between current Q table and previous Q table:  2386.286668732862\n",
      "\n",
      "Episodes ran:  1800\n",
      "Current epsilon:  0.972330665180936\n",
      "Average error between current Q table and previous Q table:  2422.6924950043976\n",
      "\n",
      "Episodes ran:  1900\n",
      "Current epsilon:  0.971358334515755\n",
      "Average error between current Q table and previous Q table:  2266.818222569017\n",
      "\n",
      "Episodes ran:  2000\n",
      "Current epsilon:  0.9703869761812393\n",
      "Average error between current Q table and previous Q table:  2129.143594090564\n",
      "\n",
      "Episodes ran:  2100\n",
      "Current epsilon:  0.969416589205058\n",
      "Average error between current Q table and previous Q table:  2142.588258486611\n",
      "\n",
      "Episodes ran:  2200\n",
      "Current epsilon:  0.968447172615853\n",
      "Average error between current Q table and previous Q table:  2070.492602353466\n",
      "\n",
      "Episodes ran:  2300\n",
      "Current epsilon:  0.9674787254432371\n",
      "Average error between current Q table and previous Q table:  2006.0289281486573\n",
      "\n",
      "Episodes ran:  2400\n",
      "Current epsilon:  0.9665112467177939\n",
      "Average error between current Q table and previous Q table:  1916.5471660020878\n",
      "\n",
      "Episodes ran:  2500\n",
      "Current epsilon:  0.9655447354710761\n",
      "Average error between current Q table and previous Q table:  1990.361290807212\n",
      "\n",
      "Episodes ran:  2600\n",
      "Current epsilon:  0.964579190735605\n",
      "Average error between current Q table and previous Q table:  1847.9277694786902\n",
      "\n",
      "Episodes ran:  2700\n",
      "Current epsilon:  0.9636146115448695\n",
      "Average error between current Q table and previous Q table:  1847.8493460432458\n",
      "\n",
      "Episodes ran:  2800\n",
      "Current epsilon:  0.9626509969333247\n",
      "Average error between current Q table and previous Q table:  1994.9454601583607\n",
      "\n",
      "Episodes ran:  2900\n",
      "Current epsilon:  0.9616883459363913\n",
      "Average error between current Q table and previous Q table:  1813.340755943874\n",
      "\n",
      "Episodes ran:  3000\n",
      "Current epsilon:  0.960726657590455\n",
      "Average score over 5k episodes:  63.916888494528244\n",
      "Average error between current Q table and previous Q table:  1766.4260563177934\n",
      "\n",
      "Episodes ran:  3100\n",
      "Current epsilon:  0.9597659309328644\n",
      "Average error between current Q table and previous Q table:  1883.7711276562943\n",
      "\n",
      "Episodes ran:  3200\n",
      "Current epsilon:  0.9588061650019316\n",
      "Average error between current Q table and previous Q table:  1911.4163694200504\n",
      "\n",
      "Episodes ran:  3300\n",
      "Current epsilon:  0.9578473588369297\n",
      "Average error between current Q table and previous Q table:  1825.5020305095404\n",
      "\n",
      "Episodes ran:  3400\n",
      "Current epsilon:  0.9568895114780928\n",
      "Average error between current Q table and previous Q table:  1861.272406727833\n",
      "\n",
      "Episodes ran:  3500\n",
      "Current epsilon:  0.9559326219666147\n",
      "Average error between current Q table and previous Q table:  1719.8979204097423\n",
      "\n",
      "Episodes ran:  3600\n",
      "Current epsilon:  0.9549766893446481\n",
      "Average error between current Q table and previous Q table:  1710.2591179434567\n",
      "\n",
      "Episodes ran:  3700\n",
      "Current epsilon:  0.9540217126553034\n",
      "Average error between current Q table and previous Q table:  1677.4041254223744\n",
      "\n",
      "Episodes ran:  3800\n",
      "Current epsilon:  0.9530676909426481\n",
      "Average error between current Q table and previous Q table:  1760.4178843310199\n",
      "\n",
      "Episodes ran:  3900\n",
      "Current epsilon:  0.9521146232517055\n",
      "Average error between current Q table and previous Q table:  1709.665155367825\n",
      "\n",
      "Episodes ran:  4000\n",
      "Current epsilon:  0.9511625086284538\n",
      "Average error between current Q table and previous Q table:  1715.409332535682\n",
      "\n",
      "Episodes ran:  4100\n",
      "Current epsilon:  0.9502113461198253\n",
      "Average error between current Q table and previous Q table:  1662.4495068957856\n",
      "\n",
      "Episodes ran:  4200\n",
      "Current epsilon:  0.9492611347737054\n",
      "Average error between current Q table and previous Q table:  1578.838661182059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_list = []     # Store scores vs number of episodes run\n",
    "episode_list = []   # Store episodes at which scores are recorded\n",
    "avg_error_list = [] # Avg error every 100 episodes\n",
    "\n",
    "while True:\n",
    "    Q_table_old = copy.deepcopy(Q_table1) # Save old Q table for comparison\n",
    "\n",
    "    epsilon *= 0.999\n",
    "    \n",
    "    num_episodes = 100 # Number of episodes to run\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        episode1 = generate_episode(Q_table1, epsilon)\n",
    "        Q_table1 = Update_Q_table(episode1, Q_table1)\n",
    "    num_keys = len(Q_table_old)\n",
    "    total_error = 0\n",
    "    \n",
    "    for key in Q_table_old.keys(): # Calculate average error\n",
    "#         print(key)\n",
    "        try:\n",
    "            for j in range(Number_of_possible_actions):\n",
    "#                 print(abs(Q_table1[key][j] - Q_table_old[key][j]))\n",
    "                total_error += abs(Q_table1[key][j] - Q_table_old[key][j])\n",
    "        except:\n",
    "            pass\n",
    "    avg_error = total_error/num_keys\n",
    "    avg_error_list += [avg_error]\n",
    "    \n",
    "    print('Episodes ran: ', Q_table1['episode_count'])\n",
    "    print('Current epsilon: ', epsilon)\n",
    "    if Q_table1['episode_count'] in range(0, 300000, 1500): # Score every x number of episodes\n",
    "        try:\n",
    "            score = score_q_table(Q_table1, num_iterations = 5000)\n",
    "            score_list += [score]\n",
    "            episode_list += [Q_table1['episode_count']]\n",
    "            print('Average score over 5k episodes: ', score)\n",
    "        except:\n",
    "            pass\n",
    "    print('Average error between current Q table and previous Q table: ', avg_error)\n",
    "    print('')\n",
    "    if epsilon <= 0.05:\n",
    "        break\n",
    "Done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.minorticks_on()\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.tick_params(which='major', length=10, width=1, direction='out')\n",
    "ax.tick_params(which='minor', length=4, width=1, direction='out')\n",
    "plot(episode_list,score_list, color = 'blue')\n",
    "legend(frameon =  0, fontsize = 20)\n",
    "ylabel('Score (5k runs)', fontsize = 20)\n",
    "xlabel('Episodes', fontsize = 20)\n",
    "xticks(fontsize = 20)\n",
    "yticks(fontsize = 20)\n",
    "show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.minorticks_on()\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.tick_params(which='major', length=10, width=1, direction='out')\n",
    "ax.tick_params(which='minor', length=4, width=1, direction='out')\n",
    "plot(episode_list,score_list, color = 'blue')\n",
    "plot(np.linspace(100,episode_list[-1],len(avg_error_list)), avg_error_list, color = 'blue')\n",
    "legend(frameon =  0, fontsize = 20)\n",
    "ylabel('Average error', fontsize = 20)\n",
    "xlabel('Episodes', fontsize = 20)\n",
    "xticks(fontsize = 20)\n",
    "yticks(fontsize = 20)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(100,episode_list[-1],len(avg_error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The method in cell below doesn't work well\n",
    "You cannot decay episilon in the subtractive fashion\n",
    "End result: average score of ~5 over 15k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = {'episode_count': 0} # Initialize empty Q table as dictionary of list where INDEX corresponds to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "epsilon = 0.90  \n",
    "\n",
    "# while True:\n",
    "#     epsilon -= 0.05\n",
    "#     print('Epsilon:',epsilon)\n",
    "#     num_episodes = 100000 # Number of episodes to run\n",
    "#     for i in range(num_episodes):\n",
    "#         episode = generate_episode(Q_table, epsilon)\n",
    "#         Q_table = Update_Q_table(episode, Q_table)\n",
    "#     print('Average score over 15k episodes:', score_q_table(Q_table, num_iterations = 15000))    \n",
    "#     if epsilon <= 0.05:\n",
    "#         break\n",
    "# Done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Number_of_possible_actions): # Test 2nd last step actions\n",
    "    total_score = 0\n",
    "    for j in range(1000):\n",
    "        new_state = transition( (1.2, 100.0, 345.6),i)\n",
    "        score = 100*new_state[0][0] - new_state[0][1]\n",
    "        total_score += score\n",
    "    print('action: ',i,'avg score: ', total_score/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current policy: [7, 5, 4, 1, 6, 6, 5, 3, 7, 0]\n",
      "Concentration of x: [0.2, 0.41, 0.8, 1.02, 0.88, 0.89, 0.99, 1.02, 0.94, 0.99, 0.85]\n",
      "Concentration of N: [150, 299.0, 223.0, 96.0, 23.0, 103.0, 108.0, 85.0, 52.0, 117.0, 11.0]\n",
      "\n",
      "Score: 74.0\n"
     ]
    }
   ],
   "source": [
    "state = (0.2,150,0) # Initial state\n",
    "x_data = [state[0]] # Store initial data\n",
    "N_data = [state[1]]\n",
    "t_data = [state[2]]\n",
    "my_policy = []\n",
    "for i in range(10): #take ten steps\n",
    "        action = np.argmax(Q_table1[state])\n",
    "        state = transition(state, action)[0]\n",
    "        my_policy += [action]\n",
    "        x_data += [state[0]]\n",
    "        N_data += [state[1]]\n",
    "        t_data += [state[2]]\n",
    "print('Current policy:', my_policy)\n",
    "print('Concentration of x:', x_data)\n",
    "print('Concentration of N:', N_data)\n",
    "print('')\n",
    "score = 100*x_data[-1] - N_data[-1]\n",
    "print('Score:', score)\n",
    "def plot(t_data, x_data, N_data):\n",
    "    plt.figure()\n",
    "    plt.plot(t_data, x_data, label = 'x')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(t_data, N_data, label = 'N')\n",
    "    plt.legend()\n",
    "    \n",
    "plot(t_data, x_data, N_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
